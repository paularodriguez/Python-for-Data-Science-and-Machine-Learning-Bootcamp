import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt

df = pd.read_csv('DATA/fake_reg.csv')
print(df.head())

#         price     feature1     feature2
# 0  461.527929   999.787558   999.766096
# 1  548.130011   998.861615  1001.042403
# 2  410.297162  1000.070267   998.844015
# 3  540.382220   999.952251  1000.440940
# 4  546.024553  1000.446011  1000.338531

# sns.pairplot(df)
# plt.show()

# We split the data into train and test datasets
from sklearn.model_selection import train_test_split

X = df[['feature1', 'feature2']].values
y = df['price'].values

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

print(X_train.shape)
# (700, 2)

print(X_test.shape)
# (300, 2)

# Now, we normalize the data to avoid issues due to the scale
from sklearn.preprocessing import MinMaxScaler

# Transform features by scaling each feature to a given range.
#
#     This estimator scales and translates each feature individually such
#     that it is in the given range on the training set, e.g. between
#     zero and one.

#     The transformation is given by:
#     X_std = (X - X.min(axis=0)) / (X.max(axis=0) - X.min(axis=0))
#     X_scaled = X_std * (max - min) + min

scaler = MinMaxScaler()
scaler.fit(X_train)

X_train = scaler.transform(X_train)
X_test = scaler.transform(X_test)

# All values are between 0 and 1
print(X_train.min())
# 0.0
print(X_train.max())
# 1.0

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense

print(help(Sequential))
# class Sequential(tensorflow.python.keras.engine.training.Model)
#  |  Sequential(layers=None, name=None)
#  |
#  |  Linear stack of layers.
#  |
#  |  Arguments:
#  |      layers: list of layers to add to the model.

# print(help(Dense))
# class Dense(tensorflow.python.keras.engine.base_layer.Layer)
#  |  Dense(units, activation=None, use_bias=True, kernel_initializer='glorot_uniform', bias_initializer='zeros', kernel_regularizer=None, bias_regularizer=None, activity_regularizer=None, kernel_constraint=None, bias_constraint=None, **kwargs)
#  |
#  |  Just your regular densely-connected NN layer.
#  |
#  |  `Dense` implements the operation:
#  |  `output = activation(dot(input, kernel) + bias)`
#  |  where `activation` is the element-wise activation function
#  |  passed as the `activation` argument, `kernel` is a weights matrix
#  |  created by the layer, and `bias` is a bias vector created by the layer
#  |  (only applicable if `use_bias` is `True`).
#  |
#  |  Note: If the input to the layer has a rank greater than 2, then
#  |  it is flattened prior to the initial dot product with `kernel`.

# Create model with Dense Layers
# Dense: Regular densely-conected NN layer
# the number of neurons - 4
# activation function - 'relu'

# model = Sequential([Dense(4, activation='relu'),
#                     Dense(2, activation='relu'),
#                     Dense(1)])

# Another way: It's better because we can easily remove layers using comments
model = Sequential()
model.add(Dense(4, activation='relu'))
model.add(Dense(4, activation='relu'))
model.add(Dense(4, activation='relu'))
model.add(Dense(1))

### Choosing an optimizer and loss

# Keep in mind what kind of problem you are trying to solve:
#
#     # For a multi-class classification problem
#     model.compile(optimizer='rmsprop',
#                   loss='categorical_crossentropy',
#                   metrics=['accuracy'])
#
#     # For a binary classification problem
#     model.compile(optimizer='rmsprop',
#                   loss='binary_crossentropy',
#                   metrics=['accuracy'])
#
#     # For a mean squared error regression problem
#     model.compile(optimizer='rmsprop',
#                   loss='mse')

model.compile(optimizer='rmsprop', loss='mse')

# * Sample: one element of a dataset.
#     * Example: one image is a sample in a convolutional network
#     * Example: one audio file is a sample for a speech recognition model
# * Batch: a set of N samples. The samples in a batch are processed independently, in parallel. If training, a batch results in only one update to the model.A batch generally approximates the distribution of the input data better than a single input. The larger the batch, the better the approximation; however, it is also true that the batch will take longer to process and will still result in only one update. For inference (evaluate/predict), it is recommended to pick a batch size that is as large as you can afford without going out of memory (since larger batches will usually result in faster evaluation/prediction).
# * Epoch: an arbitrary cutoff, generally defined as "one pass over the entire dataset", used to separate training into distinct phases, which is useful for logging and periodic evaluation.
# * When using validation_data or validation_split with the fit method of Keras models, evaluation will be run at the end of every epoch.
# * Within Keras, there is the ability to add callbacks specifically designed to be run at the end of an epoch. Examples of these are learning rate changes and model checkpointing (saving).

model.fit(x=X_train, y=y_train, epochs=250)
loss_df = pd.DataFrame(model.history.history)
loss_df.plot()
plt.show()

loss = model.evaluate(X_test, y_test, verbose=0)
# Returns the model loss
print(loss)
# 24.94885986328125

# The same with training data
print(model.evaluate(X_train, y_train, verbose=0))
# 23.7758772277832

# It's a numpy array
test_predictions = model.predict(X_test)
print(test_predictions)
# [[405.8151 ]
#  [622.11383]
#  [590.77625]
#  [571.7398 ]

# Here, we convert it to one pandas dataframe
test_predictions = pd.Series(test_predictions.reshape(300, ))
print(test_predictions.head())

# 0    405.815094
# 1    622.113831
# 2    590.776245
# 3    571.739807
# 4    368.407318
# dtype: float32

pred_df = pd.DataFrame(y_test, columns=['Test True Y'])
pred_df = pd.concat([pred_df, test_predictions], axis=1)
pred_df.columns = ['Test True Y', 'Model Predictions']
print(pred_df.head())

#    Test True Y  Model Predictions
# 0   402.296319  405.815094
# 1   624.156198  622.113831
# 2   582.455066  590.776245
# 3   578.588606  571.739807
# 4   371.224104  368.407318

# We are going to plot the values

sns.scatterplot(x='Test True Y', y='Model Predictions', data=pred_df)
plt.show()

from sklearn.metrics import mean_absolute_error, mean_squared_error

print(mean_absolute_error(pred_df['Test True Y'], pred_df['Model Predictions']))
# 4.025071752384227

print(mean_squared_error(pred_df['Test True Y'], pred_df['Model Predictions']))
# 25.06062906872388

print(df.describe())

#              price     feature1     feature2
# count  1000.000000  1000.000000  1000.000000
# mean    498.673029  1000.014171   999.979847
# std      93.785431     0.974018     0.948330
# min     223.346793   997.058347   996.995651
# 25%     433.025732   999.332068   999.316106
# 50%     502.382117  1000.009915  1000.002243
# 75%     564.921588  1000.637580  1000.645380
# max     774.407854  1003.207934  1002.666308

# The mean price is 498.673029 and our mae is 4.025, that represents about 1%
# Our mae is good
# Also, our mse is good

# PREDICT NEW DATA
new_gem = [[998, 1000]]

# 1. Our model is scaled, so we have to transform this new value
new_gem = scaler.transform(new_gem)

# 2. Do the prediction
print(model.predict(new_gem))
# [[420.2104]]

# If we want to save the model:

from tensorflow.keras.models import load_model
model.save('my_gem_model.h5')

# To get one saved model
later_model = load_model('my_gem_model.h5')