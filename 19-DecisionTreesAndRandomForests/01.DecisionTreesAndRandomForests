import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns


def separator():
    print('-' * 20)


df = pd.read_csv('kyphosis.csv')

print(df.head())
#   Kyphosis  Age  Number  Start
# 0   absent   71       3      5
# 1   absent  158       3     14
# 2  present  128       4      5
# 3   absent    2       5      1
# 4   absent    1       4     15

# Age : Age in months
# Number: Number of vertebrae involved in the operation
# Start: Number of top most vertebrae that was operated

separator()

print(df.info())

# <class 'pandas.core.frame.DataFrame'>
# RangeIndex: 81 entries, 0 to 80
# Data columns (total 4 columns):
# Kyphosis    81 non-null object
# Age         81 non-null int64
# Number      81 non-null int64
# Start       81 non-null int64
# dtypes: int64(3), object(1)
# memory usage: 2.7+ KB
# None

separator()

sns.pairplot(df, hue='Kyphosis')
plt.show()

# Prepare training and test data
from sklearn.model_selection import train_test_split

X = df.drop('Kyphosis', axis=1)
y = df['Kyphosis']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=101)

# Create model and train it
from sklearn.tree import DecisionTreeClassifier

dtree = DecisionTreeClassifier()
dtree.fit(X_train, y_train)

# Predict
predictions = dtree.predict(X_test)

# Evaluate predictions

from sklearn.metrics import classification_report, confusion_matrix

print(confusion_matrix(y_test, predictions))

# [[12  5]
#  [ 6  2]]

print(classification_report(y_test, predictions))

#               precision    recall  f1-score   support
#
#       absent       0.67      0.71      0.69        17
#      present       0.29      0.25      0.27         8
#
#     accuracy                           0.56        25
#    macro avg       0.48      0.48      0.48        25
# weighted avg       0.54      0.56      0.55        25

separator()

# Using random forests

from sklearn.ensemble import RandomForestClassifier

rfc = RandomForestClassifier(n_estimators=200)
rfc.fit(X_train, y_train)

rfc_predictions = rfc.predict(X_test)

print(confusion_matrix(y_test, rfc_predictions))
print(classification_report(y_test, rfc_predictions))

# [[17  0]
#  [ 6  2]]
#               precision    recall  f1-score   support
#
#       absent       0.74      1.00      0.85        17
#      present       1.00      0.25      0.40         8
#
#     accuracy                           0.76        25
#    macro avg       0.87      0.62      0.62        25
# weighted avg       0.82      0.76      0.71        25

# The random forest works better than one simple decision tree when we have large datasets